@misc{iso_13849_part1,
  author       = {{International Standards Organization}},
  title        = {ISO 13849-1:2015 Safety of machinery - Safety-related parts of control systems - Part 1: General principles for design},
  shorttitle   = {ISO 13849-1:2015},
  address      = {Geneva, CH},
  howpublished = {\url{https://www.iso.org/standard/69883.html}},
  month        = {12},
  year         = {2015},
  pages        = {1-86},
  language     = {en},
  number       = {ISO 13849-1:2015},
  keywords     = {norms}
}

@misc{iso_13849_part2,
  author       = {{International Standards Organization}},
  title        = {ISO 13849-2:2012 Safety of machinery -- Safety-related parts of control systems -- Part 2: Validation},
  shorttitle   = {ISO 13849-2:2012},
  address      = {Geneva, CH},
  howpublished = {\url{https://www.iso.org/standard/53640.html}},
  month        = {10},
  year         = {2012},
  pages        = {1-79},
  language     = {en},
  number       = {ISO 13849-2:2012},
  keywords     = {norms}
}

@misc{iso_12100,
  author       = {{International Standards Organization}},
  title        = {ISO 12100:2010 Safety of machinery -- General principles for design -- Risk assessment and risk reduction},
  shorttitle   = {ISO 12100:2010},
  address      = {Geneva, CH},
  howpublished = {\url{https://www.iso.org/standard/51528.html}},
  month        = {11},
  year         = {2010},
  pages        = {1-77},
  language     = {en},
  number       = {ISO 12100:2010},
  keywords     = {norms}
}

@online{ISO:online,
  author   = {{International Standards Organization}},
  title    = {About us},
  url      = {https://www.iso.org/about-us.html},
  urldate  = {2022-04-13},
  keywords = {other}
}

@online{IEC:online,
  author   = {{International Electrotechnical Comission}},
  title    = {About us},
  url      = {https://iec.ch/about-us},
  urldate  = {2022-04-13},
  keywords = {other}
}

@online{DIN:online,
  author   = {{Deutsches Institut für Normung}},
  title    = {DIN - Kurz erklärt},
  url      = {https://www.din.de/de/ueber-normen-und-standards/basiswissen},
  urldate  = {2022-04-13},
  keywords = {other}
}


@book{Isermann2010,
  title     = {Elektronisches Management Motorischer Fahrzeugantriebe},
  editor    = {Isermann, Rolf},
  publisher = {Vieweg+Teubner Verlag},
  series    = {Atz/Mtz-Fachbuch},
  edition   = 2010,
  month     = feb,
  year      = 2010,
  address   = {Wiesbaden, Germany},
  language  = {de},
  keywords  = {book}
}

@inproceedings{francke2015internet,
  title     = {Internet shopping and its impacts on mobility},
  author    = {Francke, J and Visser, J},
  booktitle = {25th World Road Congress (PIARC)},
  pages     = {2--6},
  year      = {2015},
  keywords  = {proceedings}
}

%%%%%%%%%%%%%%%% Customized part %%%%%%%%%%%%%%%%
@inproceedings{panayotov2015librispeech,
  title={Librispeech: an asr corpus based on public domain audio books},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5206--5210},
  year={2015},
  organization={IEEE}
}

@article{xlsr53,
  title={Unsupervised cross-lingual representation learning for speech recognition},
  author={Conneau, Alexis and Baevski, Alexei and Collobert, Ronan and Mohamed, Abdelrahman and Auli, Michael},
  journal={arXiv preprint arXiv:2006.13979},
  year={2020}
}

@article{wav2vec2,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

@book{ethnologue,
  title = {Ethnologue: Languages of the World},
  year = {2009},
  researchr = {https://researchr.org/publication/ethnologue},
  cites = {0},
  citedby = {0},
  edition = {Sixteenth},
  editor = {M. Paul Lewis},
  address = {Dallas, TX, USA},
  publisher = {SIL International},
}

@inproceedings{BERT,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of NAACL-HLT},
  pages={4171--4186},
  year={2019}
}

@article{latent_speech_representation,
  author    = {Dongwei Jiang and
               Xiaoning Lei and
               Wubo Li and
               Ne Luo and
               Yuxuan Hu and
               Wei Zou and
               Xiangang Li},
  title     = {Improving Transformer-based Speech Recognition Using Unsupervised
               Pre-training},
  journal   = {CoRR},
  volume    = {abs/1910.09932},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.09932},
  eprinttype = {arXiv},
  eprint    = {1910.09932},
  timestamp = {Fri, 25 Oct 2019 14:59:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-09932.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Transformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{contrastive_task,
  author    = {A{\"{a}}ron van den Oord and
               Yazhe Li and
               Oriol Vinyals},
  title     = {Representation Learning with Contrastive Predictive Coding},
  journal   = {CoRR},
  volume    = {abs/1807.03748},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.03748},
  eprinttype = {arXiv},
  eprint    = {1807.03748},
  timestamp = {Mon, 13 Aug 2018 16:48:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-03748.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gumbel_softmax,
  title={Categorical Reparametrization with Gumble-Softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  booktitle={International Conference on Learning Representations (ICLR 2017)},
  year={2017},
  organization={OpenReview. net}
}

@inproceedings{discrete_speech_units,
 author = {van den Oord, Aaron and Vinyals, Oriol and kavukcuoglu, koray},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Discrete Representation Learning},
 url = {https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{RASR-hybrid_vs_attention,
	title={{RWTH} {ASR} Systems for {LibriSpeech}: Hybrid vs {Attention}},
	author={Christoph L\"uscher and Eugen Beck and Kazuki Irie and Markus Kitza and Wilfried Michel and Albert Zeyer and Ralf Schl\"uter and Hermann Ney},
	booktitle=confInterspeech,
	address={Graz, Austria},
	month=sep,
	year={2019}
}

@inproceedings{BABEL_dataset,
  added-at = {2016-04-05T00:00:00.000+0200},
  author = {Gales, Mark J. F. and Knill, Kate M. and Ragni, Anton and Rath, Shakti P.},
  biburl = {https://www.bibsonomy.org/bibtex/2d3c601be3b65f13733ce7c0c5e380429/dblp},
  booktitle = {SLTU},
  crossref = {conf/sltu/2014},
  ee = {http://www.isca-speech.org/archive/sltu_2014/sl14_016.html},
  interhash = {4d83bf033e72fb61e4013bc240faaf02},
  intrahash = {d3c601be3b65f13733ce7c0c5e380429},
  keywords = {dblp},
  pages = {16-23},
  publisher = {ISCA},
  timestamp = {2019-04-16T11:37:57.000+0200},
  title = {Speech recognition and keyword spotting for low-resource languages: Babel project research at CUED.},
  url = {http://dblp.uni-trier.de/db/conf/sltu/sltu2014.html#GalesKRR14},
  year = 2014
}

@inproceedings{CommonVoice_dataset,
    title = "Common Voice: A Massively-Multilingual Speech Corpus",
    author = "Ardila, Rosana  and
      Branson, Megan  and
      Davis, Kelly  and
      Kohler, Michael  and
      Meyer, Josh  and
      Henretty, Michael  and
      Morais, Reuben  and
      Saunders, Lindsay  and
      Tyers, Francis  and
      Weber, Gregor",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.520",
    pages = "4218--4222",
    abstract = "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla{'}s DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 {\mbox{$\pm$}} 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{multiling_librispeech,
  title={MLS: A Large-Scale Multilingual Dataset for Speech Research},
  author={Pratap, Vineel and Xu, Qiantong and Sriram, Anuroop and Synnaeve, Gabriel and Collobert, Ronan},
  booktitle={INTERSPEECH},
  year={2020}
}

@article{facebook2020hybrid,
  title={Fast, Simpler and More Accurate Hybrid {ASR} Systems Using Wordpieces},
  author={Zhang, Frank and Wang, Yongqiang and Zhang, Xiaohui and Liu, Chunxi and Saraf, Yatharth and Zweig, Geoffrey},
  journal={arXiv preprint arXiv:2005.09150},
  year={2020}
}

@article{google2020conformer,
  title={Conformer: Convolution-augmented transformer for speech recognition},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and others},
  journal={arXiv preprint arXiv:2005.08100},
  year={2020}
}

@inproceedings{zeineldeen2022conformer,
  title={Conformer-based hybrid {ASR} system for Switchboard dataset},
  author={Zeineldeen, Mohammad and Xu, Jingjing and L{\"u}scher, Christoph and Michel, Wilfried and Gerstenberger, Alexander and Schl{\"u}ter, Ralf and Ney, Hermann},
  booktitle=confICASSP,
  pages={7437--7441},
  year={2022},
  organization={IEEE}
}

@inproceedings{vieting2021waveform,
  title={On Architectures and Training for Raw Waveform Feature Extraction in {ASR}},
  author={Vieting, Peter and L{\"u}scher, Christoph and Michel, Wilfried and Schl{\"u}ter, Ralf and Ney, Hermann},
  booktitle=confASRU,
  pages={267--274},
  year={2021},
  organization={IEEE}
}

@inproceedings{facebook2020dejavu,
  title={Deja-vu: Double feature presentation and iterated loss in deep transformer networks},
  author={Tjandra, Andros and Liu, Chunxi and Zhang, Frank and Zhang, Xiaohui and Wang, Yongqiang and Synnaeve, Gabriel and Nakamura, Satoshi and Zweig, Geoffrey},
  booktitle=confICASSP,
  pages={6899--6903},
  year={2020},
  organization={IEEE}
}

@article{mohamed2022representation_review,
  title={Self-Supervised Speech Representation Learning: A Review},
  author={Mohamed, Abdelrahman and Lee, Hung-yi and Borgholt, Lasse and Havtorn, Jakob D and Edin, Joakim and Igel, Christian and Kirchhoff, Katrin and Li, Shang-Wen and Livescu, Karen and Maal{\o}e, Lars and others},
  journal={arXiv preprint arXiv:2205.10643},
  year={2022}
}

@article{deepmind2020cpc,
  title={Learning robust and multilingual speech representations},
  author={Kawakami, Kazuya and Wang, Luyu and Dyer, Chris and Blunsom, Phil and Oord, Aaron van den},
  journal={arXiv preprint arXiv:2001.11128},
  volume={},
  year={2020}
}

@inproceedings{facebook2019wav2vec,
  title={wav2vec: Unsupervised Pre-Training for Speech Recognition},
  author={Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael},
  booktitle=confInterspeech,
  pages={3465--3469},
  address={Graz, Austria},
  month=sep,
  year={2019}
}

@inproceedings{facebook2021hubert,
  title={{HuBERT}: How much can a bad teacher benefit {ASR} pre-training?},
  author={Hsu, Wei-Ning and Tsai, Yao-Hung Hubert and Bolte, Benjamin and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  booktitle=confICASSP,
  pages={6533--6537},
  year={2021},
  organization={IEEE}
}

@article{facebook2022wav2vecaug,
  title={Wav2Vec-Aug: Improved self-supervised training with limited data},
  author={Sriram, Anuroop and Auli, Michael and Baevski, Alexei},
  journal={arXiv preprint arXiv:2206.13654},
  year={2022}
}

@inproceedings{livescu2021wav2vec_analysis,
  title={Layer-wise analysis of a self-supervised speech representation model},
  author={Pasad, Ankita and Chou, Ju-Chieh and Livescu, Karen},
  booktitle=confASRU,
  pages={914--921},
  year={2021},
  organization={IEEE}
}

@inproceedings{robust_wav2vec2,
  title={Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training},
  author={Wei-Ning Hsu and Anuroop Sriram and Alexei Baevski and Tatiana Likhomanenko and Qiantong Xu and Vineel Pratap and Jacob Kahn and Ann Lee and Ronan Collobert and Gabriel Synnaeve and Michael Auli},
  booktitle={Interspeech},
  year={2021}
}

@inproceedings{microsoft2021unispeech,
  title={Unispeech: Unified speech representation learning with labeled and unlabeled data},
  author={Wang, Chengyi and Wu, Yu and Qian, Yao and Kumatani, Kenichi and Liu, Shujie and Wei, Furu and Zeng, Michael and Huang, Xuedong},
  booktitle={International Conference on Machine Learning},
  pages={10937--10947},
  year={2021},
  organization={PMLR}
}

@article{zhang2021xlst,
  title={{XLST}: Cross-lingual self-training to learn multilingual representation for low resource speech recognition},
  author={Zhang, Zi-Qiang and Song, Yan and Wu, Ming-Hui and Fang, Xin and Dai, Li-Rong},
  journal={arXiv preprint arXiv:2103.08207},
  year={2021}
}

@inproceedings{google2022just,
  title={Joint unsupervised and supervised training for multilingual {ASR}},
  author={Bai, Junwen and Li, Bo and Zhang, Yu and Bapna, Ankur and Siddhartha, Nikhil and Sim, Khe Chai and Sainath, Tara N},
  booktitle=confICASSP,
  pages={6402--6406},
  year={2022},
  organization={IEEE}
}

@inproceedings{tuske2014multilingual,
  author={T{\"u}ske, Zolt{\'a}n and Golik, Pavel and Nolden, David and Schl{\"u}ter, Ralf and Ney, Hermann},
  title={Data Augmentation, Feature Combination, and Multilingual Neural Networks to Improve {ASR} and {KWS} Performance for Low-resource Languages},
  booktitle=confInterspeech,
  year=2014,
  pages={1420-1424},
  address={Singapore},
  month=sep
}

@InProceedings{edwards2017medicalspeech,
author="Edwards, Erik
and Salloum, Wael
and Finley, Greg P.
and Fone, James
and Cardiff, Greg
and Miller, Mark
and Suendermann-Oeft, David",
editor="Karpov, Alexey
and Potapova, Rodmonga
and Mporas, Iosif",
title="Medical Speech Recognition: Reaching Parity with Humans",
booktitle="Speech and Computer",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="512--524",
abstract="We present a speech recognition system for the medical domain whose architecture is based on a state-of-the-art stack trained on over 270 h of medical speech data and 30 million tokens of text from clinical episodes. Despite the acoustic challenges and linguistic complexity of the domain, we were able to reduce the system's word error rate to below 16{\%} in a realistic clinical use case. To further benchmark our system, we determined the human word error rate on a corpus covering a wide variety of speakers, working with multiple medical transcriptionists, and found that our speech recognition system performs on a par with humans.",
isbn="978-3-319-66429-3"
}

@inproceedings{chiu2018medconv,
    author={Chung-Cheng Chiu and Anshuman Tripathi and Katherine Chou and Chris Co and Navdeep Jaitly and Diana Jaunzeikare and Anjuli Kannan and Patrick Nguyen and Hasim Sak and Ananth Sankar and Justin Tansuwan and Nathan Wan and Yonghui Wu and Xuedong Zhang},
    title={{Speech Recognition for Medical Conversations}},
    year=2018,
    booktitle={Proc. Interspeech 2018},
    pages={2972--2976},
    doi={10.21437/Interspeech.2018-40}
}

@inproceedings{kar2021operation,
    author={Kar, Snigdhaswin and Mishra, Prabodh and Lin, Ju and Woo, Min-Jae and Deas, Nicholas and Linduff, Caleb and Niu, Sufeng and Yang, Yuzhe and McClendon, Jerome and Smith, D. Hudson and Smith, Melissa C. and Gimbel, Ronald W. and Wang, Kuang-Ching},
    booktitle={2021 International Joint Conference on Neural Networks (IJCNN)},
    title={Systematic Evaluation and Enhancement of Speech Recognition in Operational Medical Environments},
    year={2021},
    volume={},
    number={},
    pages={1-8},
    doi={10.1109/IJCNN52387.2021.9533607}
}

@inproceedings{sakti2014towards,
    title = "Towards Multilingual Conversations in the Medical Domain: Development of Multilingual Medical Data and A Network-based {ASR} System",
    author = "Sakti, Sakriani  and
              Kubo, Keigo  and
              Matsumiya, Sho  and
              Neubig, Graham  and
              Toda, Tomoki  and
              Nakamura, Satoshi  and
              Adachi, Fumihiro  and
              Isotani, Ryosuke",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/709_Paper.pdf",
    pages = "2639--2643",
    abstract = "This paper outlines the recent development on multilingual medical data and multilingual speech recognition system for network-based speech-to-speech translation in the medical domain. The overall speech-to-speech translation (S2ST) system was designed to translate spoken utterances from a given source language into a target language in order to facilitate multilingual conversations and reduce the problems caused by language barriers in medical situations. Our final system utilizes a weighted finite-state transducers with n-gram language models. Currently, the system successfully covers three languages: Japanese, English, and Chinese. The difficulties involved in connecting Japanese, English and Chinese speech recognition systems through Web servers will be discussed, and the experimental results in simulated medical conversation will also be presented.",
}

@inproceedings{mani2020towardsmedical,
    title={Towards Understanding ASR Error Correction for Medical Conversations},
    author={Anirudh Mani and Shruti Palaskar and Sandeep Konam},
    booktitle={NLPMC},
    year={2020}
}

@misc{layer_normalization,
  doi = {10.48550/ARXIV.1607.06450},
  
  url = {https://arxiv.org/abs/1607.06450},
  
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Layer Normalization},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{gelu,
  author    = {Dan Hendrycks and
               Kevin Gimpel},
  title     = {Bridging Nonlinearities and Stochastic Regularizers with Gaussian
               Error Linear Units},
  journal   = {CoRR},
  volume    = {abs/1606.08415},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.08415},
  eprinttype = {arXiv},
  eprint    = {1606.08415},
  timestamp = {Mon, 13 Aug 2018 16:46:20 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HendrycksG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@MISC{wav2vec2_towardsdatascience,
   author =       {Łukasz Sus},
   title =        {Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech Representations - Model for speech recognition explained
   },
   editor =       {towardsdatascience.com},
   month =        {June},
   year =         {2021},
   url = {https://towardsdatascience.com/wav2vec-2-0-a-framework-for-self-supervised-learning-of-speech-representations-7d3728688cae},
}

@article{product_quantization,
  title={Product Quantization for Nearest Neighbor Search},
  author={Herv{\'e} J{\'e}gou and Matthijs Douze and Cordelia Schmid},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2011},
  volume={33},
  pages={117-128}
}

@article{lample2019cross,
  title={Cross-lingual language model pretraining},
  author={Lample, Guillaume and Conneau, Alexis},
  journal={arXiv preprint arXiv:1901.07291},
  year={2019}
}

@INPROCEEDINGS{kneser_ney_lm,
  author={Kneser, R. and Ney, H.},
  booktitle={1995 International Conference on Acoustics, Speech, and Signal Processing}, 
  title={Improved backing-off for M-gram language modeling}, 
  year={1995},
  volume={1},
  number={},
  pages={181-184 vol.1},
  doi={10.1109/ICASSP.1995.479394}
}

@article{beck2019lstm,
  title={Lstm language models for lvcsr in first-pass decoding and lattice-rescoring},
  author={Beck, Eugen and Zhou, Wei and Schl{\"u}ter, Ralf and Ney, Hermann},
  journal={arXiv preprint arXiv:1907.01030},
  year={2019}
}

@article{bisani2008g2p,
    title = {Joint-sequence models for grapheme-to-phoneme conversion},
    journal = {Speech Communication},
    volume = {50},
    number = {5},
    pages = {434-451},
    year = {2008},
    issn = {0167-6393},
    doi = {https://doi.org/10.1016/j.specom.2008.01.002},
    url = {https://www.sciencedirect.com/science/article/pii/S0167639308000046},
    author = {Maximilian Bisani and Hermann Ney},
    keywords = {Grapheme-to-phoneme, Letter-to-sound, Phonemic transcription, Joint-sequence model, Pronunciation modeling},
    abstract = {Grapheme-to-phoneme conversion is the task of finding the pronunciation of a word given its written form. It has important applications in text-to-speech and speech recognition. Joint-sequence models are a simple and theoretically stringent probabilistic framework that is applicable to this problem. This article provides a self-contained and detailed description of this method. We present a novel estimation algorithm and demonstrate high accuracy on a variety of databases. Moreover, we study the impact of the maximum approximation in training and transcription, the interaction of model size parameters, n-best list generation, confidence measures, and phoneme-to-grapheme conversion. Our software implementation of the method proposed in this work is available under an Open Source license.}
}

@INPROCEEDINGS{stolcke2002srilm,
    author = {Andreas Stolcke},
    title = {SRILM -- An extensible language modeling toolkit},
    booktitle = {IN PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING (ICSLP 2002)},
    year = {2002},
    pages = {901--904},
    publisher = {},
}

@inproceedings{doetsch2016returnn,
  title={{RETURNN}: the {RWTH} extensible training framework for universal recurrent neural networks},
  author={Doetsch, Patrick and Zeyer, Albert and Voigtlaender, Paul and Kulikov, Ilya and Schl{\"u}ter, Ralf and Ney, Hermann},
  booktitle =confICASSP,
  address={New Orleans, {LA}, {USA}},
  year={2017}
}

@inproceedings{facebook2019fairseq,
  title={fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author={Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)},
  pages={48--53},
  year={2019}
}

@InProceedings {rybach2011rasr,
    author= {Rybach, David and Hahn, Stefan and Lehnen, Patrick and Nolden, David and Sundermeyer, Martin and T{\"u}ske, Zolt{\'a}n and Wiesler, Simon and Schl{\"u}ter, Ralf and Ney, Hermann},
    title= {{RASR} - The {RWTH Aachen University} Open Source Speech Recognition Toolkit},
    booktitle=confASRU,
    year= 2011,
    address= {Waikoloa, HI, USA},
    month= dec,
    booktitlelink= {http://www.asru2011.org},
    pdf = {https://www-i6.informatik.rwth-aachen.de/publications/downloader.php?id=765&row=pdf}
}

@InProceedings { schlueter:icassp07,
	author= {Schl{\"u}ter, Ralf and Bezrukov, Ilja and Wagner, Hermann and Ney, Hermann},	
	title= {Gammatone Features and Feature Combination for Large Vocabulary Speech Recognition},	
	booktitle= confICASSP,	
	year= 2007,	
	pages= {649--652},	
	address= {Honolulu, HI, USA},	
	month= apr,	
}

@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@article{park2019specaugment,
  title={Specaugment: A simple data augmentation method for automatic speech recognition},
  author={Park, Daniel S and Chan, William and Zhang, Yu and Chiu, Chung-Cheng and Zoph, Barret and Cubuk, Ekin D and Le, Quoc V},
  journal={arXiv preprint arXiv:1904.08779},
  year={2019}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997}
}

@misc{zeineldeen2022robustconformer,
  doi = {10.48550/ARXIV.2206.12955},
  
  url = {https://arxiv.org/abs/2206.12955},
  
  author = {Zeineldeen, Mohammad and Xu, Jingjing and Lüscher, Christoph and Schlüter, Ralf and Ney, Hermann},
  
  keywords = {Computation and Language (cs.CL), Audio and Speech Processing (eess.AS), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Improving the Training Recipe for a Robust Conformer-based Hybrid Model},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{focal_loss,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2980--2988},
  year={2017}
}

@inproceedings{L2_regularization,
    author = {Krogh, Anders and Hertz, John},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {J. Moody and S. Hanson and R.P. Lippmann},
    pages = {},
    publisher = {Morgan-Kaufmann},
    title = {A Simple Weight Decay Can Improve Generalization},
    url = {https://proceedings.neurips.cc/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf},
    volume = {4},
    year = {1991}
}

@inproceedings{speed_perturbation,
  title={Audio augmentation for speech recognition},
  author={Tom Ko and Vijayaditya Peddinti and Daniel Povey and Sanjeev Khudanpur},
  booktitle={INTERSPEECH},
  year={2015}
}

@article{pitch_perturbation,
  title={Data Augmentation For Children's Speech Recognition--The" Ethiopian" System For The SLT 2021 Children Speech Recognition Challenge},
  author={Chen, Guoguo and Na, Xingyu and Wang, Yongqing and Yan, Zhiyong and Zhang, Junbo and Ma, Sifan and Wang, Yujun},
  journal={arXiv preprint arXiv:2011.04547},
  year={2020}
}

@article{reverb_perturbation,
  title={JHU ASpIRE system: Robust LVCSR with TDNNS, iVector adaptation and RNN-LMS},
  author={Vijayaditya Peddinti and Guoguo Chen and Vimal Manohar and Tom Ko and Daniel Povey and Sanjeev Khudanpur},
  journal={2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},
  year={2015},
  pages={539-546}
}

@article{irie2019language,
  title={Language Modeling with Deep Transformers},
  author={Irie, Kazuki and Zeyer, Albert and Schl{\"u}ter, Ralf and Ney, Hermann},
  journal={Proc. Interspeech 2019},
  pages={3905--3909},
  year={2019}
}

@inproceedings{lee2021intermediate,
  title={Intermediate loss regularization for ctc-based speech recognition},
  author={Lee, Jaesong and Watanabe, Shinji},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6224--6228},
  year={2021},
  organization={IEEE}
}

@mics{Duy_Khanh_Finetune_Wav2vec_2_0_2022,
  author = {Duy Khanh, Le},
  doi = {10.5281/zenodo.6542357},
  license = {CC-BY-NC-4.0},
  month = {5},
  title = {{Finetune Wav2vec 2.0 For Vietnamese Speech Recognition}},
  url = {https://github.com/khanld/ASR-Wa2vec-Finetune},
  year = {2022}
}

@misc{vietai_wav2vec2_vi_2021,
  author = {Thai Binh Nguyen},
  doi = {10.5281/zenodo.5356039},
  month = {09},
  title = {{Vietnamese end-to-end speech recognition using wav2vec 2.0}},
  url = {https://github.com/vietai/ASR},
  year = {2021}
}

@inproceedings{ardila2020commonvoice,
  title={Common Voice: A Massively-Multilingual Speech Corpus},
  author={Ardila, Rosana and Branson, Megan and Davis, Kelly and Kohler, Michael and Meyer, Josh and Henretty, Michael and Morais, Reuben and Saunders, Lindsay and Tyers, Francis and Weber, Gregor},
  booktitle={Proceedings of the 12th Language Resources and Evaluation Conference},
  pages={4218--4222},
  year={2020}
}

@inproceedings{vivos_dataset,
  title={A non-expert Kaldi recipe for Vietnamese speech recognition system},
  author={Luong, Hieu-Thi and Vu, Hai-Quan},
  booktitle={Proceedings of the Third International Workshop on Worldwide Language Service Infrastructure and Second Workshop on Open Infrastructures and Analysis Frameworks for Human Language Technologies (WLSI/OIAF4HLT2016)},
  pages={51--55},
  year={2016}
}

@inproceedings{wav2vec2_conformer,
  author={Edwin G. Ng and Chung-Cheng Chiu and Yu Zhang and William Chan},
  title={{Pushing the Limits of Non-Autoregressive Speech Recognition}},
  year=2021,
  booktitle={Proc. Interspeech 2021},
  pages={3725--3729},
  doi={10.21437/Interspeech.2021-337}
}


@InProceedings{data2vec,
  title = 	 {data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language},
  author =       {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {1298--1312},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/baevski22a/baevski22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/baevski22a.html},
  abstract = 	 {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.}
}

@inproceedings{gibson2006hypothesis,
  title={Hypothesis spaces for minimum Bayes risk training in large vocabulary speech recognition.},
  author={Gibson, Matthew and Hain, Thomas},
  organization={Citeseer}
}

@misc{Aertsen_Olders_Johannesma_1981, title={Spectro-temporal receptive fields of auditory neurons in the grassfrog}, volume={39}, url={http://dx.doi.org/10.1007/BF00342772}, DOI={10.1007/bf00342772}, number={3}, journal={Biological Cybernetics}, publisher={Springer Science and Business Media LLC}, author={Aertsen, A. M. H. J. and Olders, J. H. J. and Johannesma, P. I. M.}, year={1981}, pages={195–209}, language={en} }

@misc{Greenwood_1990, title={A cochlear frequency‐position function for several species—29 years later}, volume={87}, url={http://dx.doi.org/10.1121/1.399052}, DOI={10.1121/1.399052}, number={6}, journal={The Journal of the Acoustical Society of America}, publisher={Acoustical Society of America (ASA)}, author={Greenwood, Donald D.}, year={1990}, month={Jun}, pages={2592–2605}, language={en} }

@book{Rao_KE_2017, title={Speech Recognition Using Articulatory and Excitation Source Features}, url={http://dx.doi.org/10.1007/978-3-319-49220-9}, DOI={10.1007/978-3-319-49220-9}, journal={SpringerBriefs in Electrical and Computer Engineering}, publisher={Springer International Publishing}, author={Rao, K. Sreenivasa and K E, Manjunath}, year={2017} }

@article{zhang2021dive,
  title={Dive into deep learning},
  author={Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
  journal={arXiv preprint arXiv:2106.11342},
  year={2021}
}

@inproceedings{relu,
author = {Nair, Vinod and Hinton, Geoffrey E.},
title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {807–814},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@misc{amidi2018deep,
  title={Deep Learning Cheatsheet},
  author={Amidi, Afshine and Amidi, Shervine},
  year={2018},
  publisher={CS}
}

@inproceedings{kingma2015adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={ICLR (Poster)},
  year={2015}
}

@InProceedings{xavier_init, title = 	 {Understanding the difficulty of training deep feedforward neural networks}, author = 	 {Glorot, Xavier and Bengio, Yoshua}, booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics}, pages = 	 {249--256}, year = 	 {2010}, editor = 	 {Teh, Yee Whye and Titterington, Mike}, volume = 	 {9}, series = 	 {Proceedings of Machine Learning Research}, address = 	 {Chia Laguna Resort, Sardinia, Italy}, month = 	 {13--15 May}, publisher =    {PMLR}, pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf}, url = 	 {https://proceedings.mlr.press/v9/glorot10a.html}, abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.} }

@article{dropout,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {1929–1958},
numpages = {30},
keywords = {neural networks, regularization, deep learning, model combination}
}

@article{lstm1997,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = {nov},
pages = {1735–1780},
numpages = {46}
}

@ARTICLE{brnn1997,  author={Schuster, M. and Paliwal, K.K.},  journal={IEEE Transactions on Signal Processing},   title={Bidirectional recurrent neural networks},   year={1997},  volume={45},  number={11},  pages={2673-2681},  doi={10.1109/78.650093}}

@INPROCEEDINGS{DeepResidualLearning,  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Deep Residual Learning for Image Recognition},   year={2016},  volume={},  number={},  pages={770-778},  doi={10.1109/CVPR.2016.90}}

@article{Baum1967AnIW,
  title={An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology},
  author={Leonard E. Baum and John A. Eagon},
  journal={Bulletin of the American Mathematical Society},
  year={1967},
  volume={73},
  pages={360-363}
}

@INPROCEEDINGS{Beulen98automaticquestion,
  author = {K. Beulen and H. Ney},
    title = {Automatic Question Generation For Decision Tree Based State Tying},
    booktitle = {Proceedings of the IEEE Conference on Acoustics, Speech and Signal Processing},
    year = {1998},
    pages = {805--809}
}

@article{ortmanns1997word,
  title={A word graph algorithm for large vocabulary continuous speech recognition},
  author={Ortmanns, Stefan and Ney, Hermann and Aubert, Xavier},
  journal={Computer Speech \& Language},
  volume={11},
  number={1},
  pages={43--72},
  year={1997},
  publisher={Elsevier}
}

@article{dozat2016incorporating,
  title={Incorporating nesterov momentum into adam},
  author={Dozat, Timothy},
  year={2016}
}

@InProceedings{He_2015_ICCV,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@Unpublished {luescher2022:hykist,
    author= {Lüscher, Christoph and Zeineldeen, Mohammad and Yang, Zijian and Vieting, Peter and Le-Duc, Khai and Wang, Weiyue and Schlüter, Ralf and Ney, Hermann},
    title= {Development of Hybrid ASR Systems for Low Resource Medical Domain Conversational Telephone Speech},
    note= {Baseline systems for Arabic, German, and Vietnamese for HYKIST project. Submitted to ICASSP 2023.},
    month= oct,
    year= 2022,
    url = {http://https://arxiv.org/abs/2210.13397}
}