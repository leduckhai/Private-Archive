%\section{Literature comparison}

%\input{tables/literature_compare}

%In this part we evaluate our best models compared to \cite{Duy_Khanh_Finetune_Wav2vec_2_0_2022}'s model as shown in Table \ref{literature_compare}. 
%Although our fine-tuning data is less diverse than their combination of spontaneous-reading VLSP, CommonVoice, VIVOS datasets; and our \glsxtrshort{LM} is generated by far less text data (2GB text data vs. our 500M words), we still achieved better performance on HYKIST, e.g. \glsxtrshort{WERR} 10.8\% and 13.0\% in average for monolingual and multilingual models respectively. 